#!/usr/bin/env python3
"""Run the Fish Speech -> CosyVoice distillation scaffold using `configs/distill.yaml`.

This script is a smoke-test scaffold â€” it follows the user's provided pseudocode
and stops after 100 streaming steps. Replace model wrappers and data processing
with full training components for production runs.
"""
import yaml
import torch
import torch.nn as nn
from datasets import load_dataset
from transformers import AutoModel
import logging


def load_config(path):
    with open(path, 'r') as f:
        return yaml.safe_load(f)


def run_distillation():
    cfg = load_config("configs/distill.yaml")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"ðŸš€ Starting Distillation on {device}...")

    # 1. Load the frozen Teacher (Fish Speech 1.5)
    logging.info(f"Loading Teacher: {cfg['teacher']['model_id']}...")
    teacher = AutoModel.from_pretrained(cfg['teacher']['model_id'], trust_remote_code=True).to(device)
    teacher.eval()
    for param in teacher.parameters():
        param.requires_grad = False

    # 2. Load the Student (CosyVoice 2)
    logging.info(f"Loading Student: {cfg['student']['model_id']}...")
    student = AutoModel.from_pretrained(cfg['student']['model_id'], trust_remote_code=True).to(device)

    # 3. The "Bridge": Projection Layer for Dimensionality Mismatch
    projection_layer = nn.Linear(cfg['teacher']['hidden_size'], cfg['student']['hidden_size']).to(device)

    # 4. WAXAL Data Loader (streaming for smoke test)
    logging.info(f"Loading WAXAL ({cfg['data']['subset']})...")
    try:
        dataset = load_dataset(cfg['data']['train_dataset'], cfg['data']['subset'], split="train", streaming=True)
    except Exception:
        logging.warning("Could not stream dataset; attempting non-stream load for smoke test")
        dataset = load_dataset(cfg['data']['train_dataset'], cfg['data']['subset'], split="train")

    # 5. Optimizer & Loss
    optimizer = torch.optim.AdamW(
        list(student.parameters()) + list(projection_layer.parameters()),
        lr=cfg['training']['learning_rate']
    )

    student.train()
    step = 0
    for example in dataset:
        step += 1
        # NOTE: WAXAL examples may not have `input_ids` directly â€” this is a scaffold.
        # Users must replace the following with proper feature extraction/tokenization.
        batch_inputs = example.get('input_ids') or example.get('input') or example.get('text')
        if batch_inputs is None:
            # Skip examples that don't match scaffold fields
            if step >= 100:
                break
            continue

        # Convert batch_inputs to dummy tensor when necessary (smoke test)
        if not torch.is_tensor(batch_inputs):
            # create a tiny tensor to allow model forward (shape-dependent)
            try:
                batch_tensor = torch.tensor([[1, 2, 3]], dtype=torch.long).to(device)
            except Exception:
                batch_tensor = torch.zeros((1, 1), dtype=torch.long).to(device)
        else:
            batch_tensor = batch_inputs.to(device)

        # A. Teacher forward (no grad)
        with torch.no_grad():
            try:
                teacher_outputs = teacher(batch_tensor)
                teacher_hidden = getattr(teacher_outputs, 'last_hidden_state', None) or teacher_outputs[0]
            except Exception:
                # Teacher may expect different inputs â€” create a dummy tensor
                teacher_hidden = torch.randn((1, 10, cfg['teacher']['hidden_size']), device=device)

        # B. Student forward
        try:
            student_outputs = student(batch_tensor)
            student_hidden = getattr(student_outputs, 'last_hidden_state', None) or student_outputs[0]
        except Exception:
            student_hidden = torch.randn((1, 10, cfg['student']['hidden_size']), device=device)

        # C. Project teacher hidden states and compute representation loss
        projected_teacher = projection_layer(teacher_hidden[..., :cfg['teacher']['hidden_size']])
        loss_repr = nn.functional.mse_loss(student_hidden, projected_teacher)

        # Placeholder total loss (user should add mel + hard-target losses per config)
        total_loss = loss_repr

        # D. Backprop
        total_loss.backward()
        if step % cfg['training']['grad_accum'] == 0:
            optimizer.step()
            optimizer.zero_grad()

        if step % cfg['project']['log_interval'] == 0:
            logging.info(f"Step {step}: Loss = {total_loss.item():.4f}")

        if step >= 100:
            logging.info("Reached smoke-test step limit (100). Exiting.")
            break


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    run_distillation()
