{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Swahili Tokenizer on Colab GPU\n",
    "\n",
    "This notebook:\n",
    "1. Sets up Colab GPU\n",
    "2. Loads data from archive.zip\n",
    "3. Trains the tokenizer\n",
    "4. Saves the model to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Msingi-AI/Sauti-Ya-Kenya.git\n",
    "%cd Sauti-Ya-Kenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install requirements\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up directories\n",
    "import os\n",
    "\n",
    "# Drive paths\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/Sauti-Ya-Kenya\"\n",
    "DRIVE_DATA = os.path.join(DRIVE_ROOT, \"data\")\n",
    "DRIVE_ARCHIVE = os.path.join(DRIVE_DATA, \"archive.zip\")\n",
    "DRIVE_OUTPUT = os.path.join(DRIVE_ROOT, \"tokenizer\")\n",
    "\n",
    "# Local paths\n",
    "LOCAL_DATA = \"data\"\n",
    "LOCAL_TEXT = os.path.join(LOCAL_DATA, \"text\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(DRIVE_DATA, exist_ok=True)\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "os.makedirs(LOCAL_TEXT, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract archive\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def extract_text_files(zip_path, extract_to):\n",
    "    \"\"\"Extract .txt files from zip to target directory\"\"\"\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Get list of .txt files\n",
    "        txt_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
    "        \n",
    "        # Extract them\n",
    "        for txt_file in txt_files:\n",
    "            zip_ref.extract(txt_file, extract_to)\n",
    "            \n",
    "        print(f\"Extracted {len(txt_files)} text files\")\n",
    "        return txt_files\n",
    "\n",
    "# Clear previous extracts\n",
    "if os.path.exists(LOCAL_TEXT):\n",
    "    shutil.rmtree(LOCAL_TEXT)\n",
    "os.makedirs(LOCAL_TEXT)\n",
    "\n",
    "# Extract new files\n",
    "extracted_files = extract_text_files(DRIVE_ARCHIVE, LOCAL_TEXT)\n",
    "\n",
    "# Show first few files\n",
    "print(\"\\nFirst few extracted files:\")\n",
    "for f in extracted_files[:5]:\n",
    "    print(f\"- {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train tokenizer\n",
    "!python src/train_tokenizer.py \\\n",
    "    --data-dir $LOCAL_TEXT \\\n",
    "    --output-dir $DRIVE_OUTPUT \\\n",
    "    --vocab-size 8000 \\\n",
    "    --min-length 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the trained tokenizer\n",
    "from src.preprocessor import SwahiliTokenizer, TextPreprocessor\n",
    "\n",
    "tokenizer = SwahiliTokenizer.load(f\"{DRIVE_OUTPUT}/tokenizer.model\")\n",
    "preprocessor = TextPreprocessor(tokenizer)\n",
    "\n",
    "test_texts = [\n",
    "    \"Habari yako! How are you doing leo?\",\n",
    "    \"Niko sawa sana, asante.\",\n",
    "    \"Tutaonana kesho asubuhi at 9 AM.\",\n",
    "    \"The weather ni nzuri sana today!\"\n",
    "]\n",
    "\n",
    "print(\"Testing tokenizer on code-switched sentences:\\n\")\n",
    "for text in test_texts:\n",
    "    tokens = preprocessor.process_text(text)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Tokens: {tokens.token_ids.tolist()}\")\n",
    "    print(f\"Languages: {tokens.languages}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify saved files\n",
    "print(\"Files saved in Google Drive:\")\n",
    "!ls -lh $DRIVE_OUTPUT"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
