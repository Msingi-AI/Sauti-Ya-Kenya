{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauti ya Kenya - TTS Training\n",
    "\n",
    "This notebook trains the Kenyan Swahili TTS model using FastSpeech 2 architecture with memory optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone repository and install dependencies\n",
    "!git clone https://github.com/Msingi-AI/Sauti-Ya-Kenya.git\n",
    "%cd Sauti-Ya-Kenya\n",
    "!pip install -r requirements.txt\n",
    "!pip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Copy data from Drive (if stored there)\n",
    "!mkdir -p data/processed\n",
    "!cp -r \"/content/drive/MyDrive/Sauti-Ya-Kenya/data/processed/*\" data/processed/\n",
    "!cp -r \"/content/drive/MyDrive/Sauti-Ya-Kenya/data/tokenizer\" data/tokenizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from src.preprocessor import SwahiliTokenizer\n",
    "from src.model import FastSpeech2\n",
    "from src.dataset import TTSDataset\n",
    "from src.config import ModelConfig\n",
    "\n",
    "# Memory optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Load config\n",
    "config = ModelConfig()\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = SwahiliTokenizer(vocab_size=8000)\n",
    "tokenizer.load('data/tokenizer/tokenizer.model')\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TTSDataset(\n",
    "    data_dir='data/processed',\n",
    "    metadata_file='data/processed/metadata.csv',\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = FastSpeech2(config).cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with memory optimizations\n",
    "def train_epoch(model, dataloader, optimizer, scaler, grad_accum_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Move batch to GPU\n",
    "        text_ids = batch['text_ids'].cuda()\n",
    "        mel_target = batch['mel_target'].cuda()\n",
    "        duration = batch['duration'].cuda()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with autocast():\n",
    "            mel_output, duration_pred = model(text_ids)\n",
    "            loss = model.loss(mel_output, mel_target, duration_pred, duration)\n",
    "            loss = loss / grad_accum_steps\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % grad_accum_steps == 0:\n",
    "            # Unscale gradients for clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 10 == 0:\n",
    "            print(f'Batch {i}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "num_epochs = 100\n",
    "save_interval = 10\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(model, dataloader, optimizer, scaler)\n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }\n",
    "        torch.save(checkpoint, f'{checkpoint_dir}/checkpoint_epoch_{epoch}.pt')\n",
    "        \n",
    "        # Save to Drive\n",
    "        drive_path = '/content/drive/MyDrive/Sauti-Ya-Kenya/checkpoints'\n",
    "        os.makedirs(drive_path, exist_ok=True)\n",
    "        torch.save(checkpoint, f'{drive_path}/checkpoint_epoch_{epoch}.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
