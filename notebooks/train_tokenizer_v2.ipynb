{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Swahili Tokenizer on Colab GPU\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload your `archive.zip` using the file upload widget below\n",
    "2. The trained tokenizer will be saved to Google Drive\n",
    "\n",
    "Note: This notebook uses Colab's T4 GPU for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Upload archive.zip\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Please upload your archive.zip file when prompted...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'archive.zip' not in uploaded:\n",
    "    raise ValueError(\"Please upload a file named 'archive.zip'\")\n",
    "\n",
    "print(\"\\nUpload successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Msingi-AI/Sauti-Ya-Kenya.git\n",
    "%cd Sauti-Ya-Kenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install requirements\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up directories\n",
    "import os\n",
    "\n",
    "# Drive paths for output\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/Sauti-Ya-Kenya\"\n",
    "DRIVE_OUTPUT = os.path.join(DRIVE_ROOT, \"tokenizer\")\n",
    "\n",
    "# Local paths\n",
    "LOCAL_DATA = \"data\"\n",
    "LOCAL_TEXT = os.path.join(LOCAL_DATA, \"text\")\n",
    "LOCAL_ARCHIVE = \"archive.zip\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "os.makedirs(LOCAL_TEXT, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract archive\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def extract_text_files(zip_path, extract_to):\n",
    "    \"\"\"Extract .txt files from zip to target directory\"\"\"\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Get list of .txt files\n",
    "        txt_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
    "        \n",
    "        if not txt_files:\n",
    "            raise ValueError(\"No .txt files found in archive.zip!\")\n",
    "        \n",
    "        # Extract them\n",
    "        for txt_file in txt_files:\n",
    "            zip_ref.extract(txt_file, extract_to)\n",
    "            \n",
    "        print(f\"Extracted {len(txt_files)} text files\")\n",
    "        return txt_files\n",
    "\n",
    "# Clear previous extracts\n",
    "if os.path.exists(LOCAL_TEXT):\n",
    "    shutil.rmtree(LOCAL_TEXT)\n",
    "os.makedirs(LOCAL_TEXT)\n",
    "\n",
    "try:\n",
    "    # Extract new files\n",
    "    extracted_files = extract_text_files(LOCAL_ARCHIVE, LOCAL_TEXT)\n",
    "    \n",
    "    # Show first few files\n",
    "    print(\"\\nFirst few extracted files:\")\n",
    "    for f in extracted_files[:5]:\n",
    "        print(f\"- {f}\")\n",
    "        \n",
    "    # Show total token count\n",
    "    total_tokens = 0\n",
    "    for txt_file in extracted_files:\n",
    "        with open(os.path.join(LOCAL_TEXT, txt_file), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            total_tokens += len(text.split())\n",
    "    \n",
    "    print(f\"\\nTotal approximate tokens: {total_tokens:,}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nPlease make sure:\")\n",
    "    print(\"1. You uploaded archive.zip using the file upload widget above\")\n",
    "    print(\"2. The archive contains .txt files\")\n",
    "    print(\"3. The text files are encoded in UTF-8\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train tokenizer\n",
    "!python src/train_tokenizer.py \\\n",
    "    --data-dir $LOCAL_TEXT \\\n",
    "    --output-dir $DRIVE_OUTPUT \\\n",
    "    --vocab-size 8000 \\\n",
    "    --min-length 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the trained tokenizer\n",
    "from src.preprocessor import SwahiliTokenizer, TextPreprocessor\n",
    "\n",
    "try:\n",
    "    tokenizer = SwahiliTokenizer.load(f\"{DRIVE_OUTPUT}/tokenizer.model\")\n",
    "    preprocessor = TextPreprocessor(tokenizer)\n",
    "\n",
    "    test_texts = [\n",
    "        \"Habari yako! How are you doing leo?\",\n",
    "        \"Niko sawa sana, asante.\",\n",
    "        \"Tutaonana kesho asubuhi at 9 AM.\",\n",
    "        \"The weather ni nzuri sana today!\"\n",
    "    ]\n",
    "\n",
    "    print(\"Testing tokenizer on code-switched sentences:\\n\")\n",
    "    for text in test_texts:\n",
    "        tokens = preprocessor.process_text(text)\n",
    "        print(f\"Input: {text}\")\n",
    "        print(f\"Tokens: {tokens.token_ids.tolist()}\")\n",
    "        print(f\"Languages: {tokens.languages}\\n\")\n",
    "        \n",
    "    print(\"‚úÖ Tokenizer trained and saved successfully!\")\n",
    "    print(f\"üìÅ Model saved to: {DRIVE_OUTPUT}/tokenizer.model\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing tokenizer: {str(e)}\")\n",
    "    print(\"Please check the training output above for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify saved files\n",
    "print(\"Files saved in Google Drive:\")\n",
    "!ls -lh $DRIVE_OUTPUT"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
